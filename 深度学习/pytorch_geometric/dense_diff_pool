# 可微池化
# paper: https://arxiv.org/pdf/1806.08804.pdf
# official code: https://github.com/RexYing/diffpool


EPS = 1e-15


def dense_diff_pool(x, adj, s, mask=None, normalize=True):
    # x: [B, N, D] or [N, D], 当前GCN层的节点特征
    # adj: [B, N, D] or [N, D], 当前层的邻接矩阵
    # s: [B, N_i, N_i+1] or [N_i, N_i+1], 当前层到下一层的分配矩阵
    # 具体情况见该论文
    
    # 在batch维度进行扩充
    x = x.unsqueeze(0) if x.dim() == 2 else x
    adj = adj.unsqueeze(0) if adj.dim() == 2 else adj
    s = s.unsqueeze(0) if s.dim() == 2 else s

    batch_size, num_nodes, _ = x.size()

    s = torch.softmax(s, dim=-1)

    if mask is not None: # 屏蔽特定节点（好像可以用来屏蔽一下stuff类，如果不用屏蔽stuff类，那么现在的datamapper好像需要更改，回去需要进一步检查）
        mask = mask.view(batch_size, num_nodes, 1).to(x.dtype)
        x, s = x * mask, s * mask

    out = torch.matmul(s.transpose(1, 2), x)  # [B, N_i+1, N_i] * [B, N_i, D] -> [B, N_i+1, D]
    
    # S.T*A*S
    # 从左到右：
    # [B, N_i+1, N_i] * [B, N_i, N_i] -> [B, N_i+1, N_i]
    # [B, N_i+1, N_i] * [B, N_i, N_i+1] -> [B, N_i+1, N_i+1]
    out_adj = torch.matmul(torch.matmul(s.transpose(1, 2), adj), s)
    
    # 下面两个loss是防止训练早期整体模型掉入局部最优
    link_loss = adj - torch.matmul(s, s.transpose(1, 2))  # link prediction objective loss
    link_loss = torch.norm(link_loss, p=2)
    if normalize is True:
        link_loss = link_loss / adj.numel()
    
    # 这个是比较好理解的：对于当前层的每个节点N_ii, 其只会被“聚类”到下一层的一个N_i+1k中
    # 因此，S中的每一行(Si)应该是一个类one-hot形式
    ent_loss = (-s * torch.log(s + EPS)).sum(dim=-1).mean()  # entropy regularization loss
    

    return out, out_adj, link_loss, ent_loss
